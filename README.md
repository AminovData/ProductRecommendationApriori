# README

# E-commerce Data Analysis and Association Rule Mining

The present README is divided into two parts: instructions for running the file, and interpretations and assumptions necessary for understanding the background of the basket analysis code.

## Instructions to Run the Code

1. **Setup**:
    
    Install the following packages: 
    
    - Ensure you have Python installed with the necessary libraries (`pandas`, `numpy`, `matplotlib`, `seaborn`, `mlxtend`).
    - Download the dataset from Kaggle and place it in the working directory.
        
        [https://www.kaggle.com/datasets/carrie1/ecommerce-data](https://www.kaggle.com/datasets/carrie1/ecommerce-data)
        
        ![Untitled](README%20f7f3dd4036a74f70aef05a9b2deee526/b5660f28-9f46-4234-881a-80d560d7532c.png)
        
    - The file should be saved in the same directory as the Python file.
2. **Execution**:
    - Run the Jupyter Notebook or Python script that contains the code.
        
        The code should be run in the following order:
        
        1. EDA - To Uncover the following:
            - Missing Values
            - Duplicated Rows
            - Identified Inconsistent Data
            - Identified Service Related Data
            - Identified Redundant Data
        2. Data Cleaning
        3. Data Preprocessing
        4. Implementation of Apriori
        5. Rule-Based Analysis
    - The results will be displayed as plots and tables within the notebook or as output in the terminal.

# Overview

# Apriori Algorithm

## Methodology

The Apriori algorithm is a fundamental algorithm in association rule mining. It is used to    generate association rules which used  for identifying the frequent itemsets in a certain dataset. The Apriori algorithm works in the following two main steps:

1. **Frequent Itemset Generation**: Find all groups of products that appear together frequently enough within the all transactions. "Support" means the percentage of transactions that include a specific group of products. First it starts looking at single item but then combines them into larger groups removing those that don't appear frequently enough according to the threshold.
2. **Rule Generation**: Create association rules from the those frequent groups of products that were found in the **Frequent Itemset Generation step** . An association rule it is like "If A is bought, B is also likely to be bought." (A and B are groups of products in this case). The quality of these rules is measured by confidence and lift. Confidence shows how often B is bought when A is bought. Lift shows how strong  the association between A and B then  if they were bought independently. The rule can be not only between A and B product but between Itemset A and item 

# Rule-Based Analysis

## Methodology

Rule-based analysis involves examining the association rules generated by the Apriori algorithm. Each rule is of the form A→B, where A and B are itemsets. The analysis focuses on the following metrics:

1. **Support**: The proportion of transactions that contain the itemset. A higher support indicates that the itemset is frequently purchased.
2. **Confidence**: The likelihood that a transaction containing A also contains B. Higher confidence indicates a stronger association between A and B.
3. **Lift**: The ratio of the observed support to the expected support if A and B were independent. A lift greater than 1 indicates a positive association between A and B.

The project 

This project analyzes an e-commerce dataset to uncover meaningful association rules using the Apriori algorithm. 

The analysis involves data cleaning, preprocessing, and the application of association rule mining to identify patterns in customer purchases.

![Untitled](README%20f7f3dd4036a74f70aef05a9b2deee526/Untitled.png)

## **Dataset Metadat:**

**Columns Description:**

- **InvoiceNo**: A 6-digit number uniquely assigned to each transaction. A 'c' prefix indicates a cancellation.
- **StockCode**: A unique identifier for each product.
- **Description**: The name or brief description of the product.
- **Quantity**: The number of units of the product sold.
- **InvoiceDate**: The date and time when the transaction was made.
- **UnitPrice**: The price per unit of the product in sterling.
- **CustomerID**: A unique identifier for each customer.
- **Country**: The country where the customer resides.

## Exploratory Data Analysis (EDA)

Plots and statistical description summaries were used to understand the distribution and characteristics of the data and to uncover inconsistencies within the data..

- Summary of the EDA:
    - Found Missing Values
    - Found Duplicated Rows
    - Identified Inconsistent Data
    - Identified Service Related Data
    - Identified Redundant Data

## Data Cleaning and Preprocessing

- **Handling Missing Values**:
    - Missing values in the `Description` column are filled using the `StockCode`, which is a unique identifier for each product.
    - Missing values in the `CustomerID` column are retained but flagged as '`Unknown`' for easier identification and filtering.
- **Removing Duplicates**: Duplicate rows are identified and removed.
- **Handling Negative Values**:
    - Transactions with negative `Quantity` and `UnitPrice` are considered refunds or adjustments and are flagged accordingly.
- **Filtered Out  Non-Product Data**:
    
    ```python
    for incon_str in tqdm(inconsistent_data):
        df = df[df['Description'] != incon_str]
    ```
    
- **Filtered Out Transactions  with low Product Counts**
    - Transactions with fewer than 47 product counts (0.9 quantile) are removed for the following reasons:
        - Computational resources that we have available for use (the main reason).
        - We want to concentrate on the top 10% of invoices in terms of the number of items.
    
    ```python
    tmp = df[["InvoiceNo", "Description"]].groupby("InvoiceNo").count()
    invoice_threshold = tmp['Description'].quantile(0.90)
    list_interest = tmp[tmp["Description"]>invoice_threshold].index
    tmp = df[df["InvoiceNo"].isin(list_interest)]
    ```
    
- **Filtered Out Products(Descriptions) with low Frequency**
    - Products with lower frequency are removed, as they will not have much association with other products. Therefore, we are setting the threshold at the 0.90% (177) quantile and focusing on the products that are in the top 10% frequency
    
    ```python
    
    product_count = tmp[["InvoiceNo", "Description"]].groupby("Description").count() 
    description_threshold = product_count["InvoiceNo"].quantile(0.90)
    list_prod = product_count[product_count["InvoiceNo"] > description_threshold].index
    final_df = tmp[tmp["Description"].isin(list_prod)]
    ```
    

- **One Hot Encoding of the Data**
    
    ```python
    data = final_df.copy()
    
    transactions = data.groupby(['InvoiceNo', 'InvoiceDate'])['Description'].apply(list).reset_index(drop=True)
    
    unique_items = list(set(item for sublist in transactions for item in sublist))
    encoded_df = pd.DataFrame(0, index=range(len(transactions)), columns=unique_items)
    
    for i, transaction in enumerate(transactions):
        for item in transaction:
            encoded_df.at[i, item] = 1
    
    item_frequencies = encoded_df.sum().sort_values(ascending=False)
    print("Item Frequencies:")
    print(item_frequencies)
    ```
    
    The Apriori algorithm requires input data in a binary matrix format, which is obtained using one-hot encoding to count the occurrences of itemsets.
    
    Result:
    
    ![Untitled](README%20f7f3dd4036a74f70aef05a9b2deee526/Untitled%201.png)
    
    # Association Rule Mining
    
    The Apriori algorithm is applied to identify frequent itemsets and derive association rules. 
    
    The following steps are used: 
    
    **Setting Parameters:**
    
    - `min_support`: The minimum support threshold.
    - `min_confidence`: The minimum confidence threshold.
    - `min_lift`: The minimum lift threshold to ensure meaningful associations.
        
        ```python
        parameters = [
            {'min_support': 0.05, 'min_confidence': 0.5, 'min_lift': 1.5},
            {'min_support': 0.05, 'min_confidence': 0.6, 'min_lift': 1.5},
            {'min_support': 0.05, 'min_confidence': 0.7, 'min_lift': 1.5},
            {'min_support': 0.05, 'min_confidence': 0.8, 'min_lift': 1.5},
            {'min_support': 0.1, 'min_confidence': 0.5, 'min_lift': 1.5},
            {'min_support': 0.1, 'min_confidence': 0.6, 'min_lift': 1.5},
            {'min_support': 0.1, 'min_confidence': 0.7, 'min_lift': 1.5},
            {'min_support': 0.1, 'min_confidence': 0.8, 'min_lift': 1.5},
            {'min_support': 0.15, 'min_confidence': 0.5, 'min_lift': 1.5},
            {'min_support': 0.15, 'min_confidence': 0.6, 'min_lift': 1.5},
            {'min_support': 0.15, 'min_confidence': 0.7, 'min_lift': 1.5},
            {'min_support': 0.15, 'min_confidence': 0.8, 'min_lift': 1.5}
        ]
        ```
        
    
    **Filtered Results:** 
    
    - Rules were filtered  using `min_lift` and `min_confidence.`
        
        ```python
        
        def select_top_singular_and_multi_antecedents(rules_df, min_lift=1.5, min_confidence=0.5, use_conviction=False, min_conviction=1.5, top_n=10):
        
            ......................................
        
        def recommend_items(cart_items, rules_df):
            .....................................
        
        rules = aprior_func(encoded_df, min_support = 0.05, min_confidence = 0.5 )
        
        cart_items = {'SET/6 RED SPOTTY PAPER CUPS'}
        recommend_items(cart_items, rules)
        ```
        
    
    **Evaluation**
    
    - The rules of different parameter setting were evaluated using:
        - Top singular and multi products antecedents
            - Singular
                
                ![Untitled](README%20f7f3dd4036a74f70aef05a9b2deee526/Untitled%202.png)
                
            - Multi
                
                ![Untitled](README%20f7f3dd4036a74f70aef05a9b2deee526/Untitled%203.png)
                
        - Visualisations
            
            ![Untitled](README%20f7f3dd4036a74f70aef05a9b2deee526/Untitled%204.png)
            
            ![Untitled](README%20f7f3dd4036a74f70aef05a9b2deee526/Untitled%205.png)
            
            ![Untitled](README%20f7f3dd4036a74f70aef05a9b2deee526/Untitled%206.png)
            
            ![Untitled](README%20f7f3dd4036a74f70aef05a9b2deee526/Untitled%207.png)
            
    
    # Final Interpretation:
    
    **Graph with Support: 0.05, Confidence: 0.5**
    
    - According to the graph, the low support and moderate confidence helped to identify a large number of association rules.
    The lift (used as the hue in the graph) shows there are a lot of weak and moderate associations, and some associations are strong.
    
    **Graph with Support: 0.05, Confidence: 0.7**
    
    - It shows that increasing the confidence filters out some rules, but in general, the number of rules remains high due to the low support threshold.
    By increasing the confidence, we got rid of the rules that have reliability lower than 0.7.
    
    **Graph with Support: 0.1, Confidence: 0.7**
    
    - In this case, increasing the support while keeping the confidence level at 0.7, we have created a more strict rule which filtered out most of the rules, keeping only the top frequent ones.
    For this dataset, such a setting gets rid of most of the association rules, which is not useful.
    Also, increasing the confidence level does not have a dramatic effect on the top list as it only filters out the rules on the bottom.
    This means setting the confidence level that interests us and playing with support can help uncover a new set of rules which might help to uncover hidden patterns of association rules.
    
    **The last graph with Support: 0.06, Confidence: 0.7** and the Top Singular and Top Multi
    
    proves that increasing the support helps to get more frequent association rules and uncover other rules patterns.
